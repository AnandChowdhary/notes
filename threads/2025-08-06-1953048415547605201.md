---
date: 2025-08-06T11:00:20.197Z
url: https://x.com/AnandChowdhary/status/1953048415547605201
---

OpenAI's latest release of GPT-OSS models, gpt-oss-120b and gpt-oss-20b, marks a big shift towards open-weight AI. This means developers can now tap into some seriously advanced reasoning capabilities.  
  
Both models use a Mixture-of-Experts setup, optimizing computational efficiency by activating only parts of the parameters per token. The 120b model handles about 5.1 billion parameters per token, while 20b works with 3.6 billion. Impressive, right?  
  
Quantized using MXFP4, the 120b model fits on an 80GB GPU. Meanwhile, the 20b model is perfect for a 16GB memory space, making accessibility a big win here!  
  
You can work with context lengths up to 128,000 tokens thanks to Rotary Position Embeddings (RoPE). Designed for all kinds of workflows, these models enable tasks like web search and Python code execution. Plus, you can adjust reasoning effort - how cool is that?  
  
Benchmark alert: gpt-oss-120b matches OpenAI's o4-mini model, even surpasses DeepSeek's R1 for coding tasks. For those with less power-hungry needs, the 20b model provides stellar performance while being nifty on resources - great for personal devices!  
  
By releasing these under the Apache 2.0 license, OpenAI gives developers the power to fine-tune and deploy as they wish. This move is key for innovation and transparency in AI development.  
  
With this step, OpenAI is not just pushing technological boundaries but is reshaping the conversation around open-weight models in AI. Exciting times ahead for developers and the community!